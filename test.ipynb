{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asyncio in c:\\users\\danie\\onedrive\\7 - estudos\\3 - mba usp 2022\\6 - tcc\\code\\govbrprocurement\\venv\\lib\\site-packages (3.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping.crawlers.crawler_oxy\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ironpdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mScraping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m cw \u001b[38;5;241m=\u001b[39m crawlers\u001b[38;5;241m.\u001b[39mCrawlerMaster(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcmpg.oxy.elotech.com.br\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4119905\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPonta Grossa/PR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m rows \u001b[38;5;241m=\u001b[39m cw\u001b[38;5;241m.\u001b[39mquery_produrements()\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\Scraping\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrawlers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\Scraping\\crawlers\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrawler_oxy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlerOxy\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrawler_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlerBase\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrawler_master\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlerMaster\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\Scraping\\crawlers\\crawler_oxy.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrawler_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlerBase\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrawler_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlerBase\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCrawlerOxy\u001b[39;00m(CrawlerBase):\n\u001b[0;32m     19\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOxy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\Scraping\\crawlers\\crawler_base.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOCR\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_text\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOCR\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_text\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCrawlerBase\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, crawlerid, url, idcity, city):\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\OCR\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_pdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_text\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\OCR\\convert_pdf.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#V2:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mironpdf\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maiopytesseract\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m config_file:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ironpdf'"
     ]
    }
   ],
   "source": [
    "from Scraping import *\n",
    "\n",
    "cw = crawlers.CrawlerMaster(1, 'cmpg.oxy.elotech.com.br', 4119905, 'Ponta Grossa/PR')\n",
    "rows = cw.query_produrements()\n",
    "\n",
    "urls = []\n",
    "for row in rows:\n",
    "    url = 'https://{base}/portaltransparencia-api/api/licitacoes/arquivos?entidade=1&exercicio={year}&tipoLicitacao={type}&licitacao={num}'.format(\n",
    "        base=cw._url,\n",
    "        year=row[2],\n",
    "        type=row[3],\n",
    "        num=row[4]\n",
    "    )\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "responses = []\n",
    "\n",
    "async def fetch(client, url):\n",
    "    async with client.get(url) as resp:\n",
    "        assert resp.status == 200\n",
    "        html = await resp.json()\n",
    "        responses.append(html)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as client:\n",
    "        await asyncio.gather(*[\n",
    "            asyncio.ensure_future(fetch(client, url))\n",
    "            for url in urls\n",
    "        ])\n",
    "\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest-asyncio in c:\\users\\danie\\onedrive\\7 - estudos\\3 - mba usp 2022\\6 - tcc\\code\\govbrprocurement\\venv\\lib\\site-packages (1.5.8)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "i=0\n",
    "to_insert = []\n",
    "for response in responses:\n",
    "    i += 1\n",
    "    for item in response:\n",
    "        id = str(item['licitacao']) + '/' + str(item['exercicio']) + '/' + str(item['tipoLicitacao'])\n",
    "        to_insert.append((cw._id_ibge, id, item['arquivoCompras']['idArquivo'], os.path.join(\"/\" + str(cw._id_ibge), id, str(item['arquivoCompras']['idArquivo']) + \".pdf\"), item['arquivoCompras']['nomeArquivo'], item['arquivoCompras']['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert = \"INSERT OR IGNORE INTO FILES (ibge, procurement_id, file_id, path, OCR_RAW, name, date) VALUES (?, ?, ?, ?, '', ?, ?)\"\n",
    "con = sqlite3.connect(\"scraped.db\", timeout=50)\n",
    "cursor = con.cursor()\n",
    "cursor.executemany(insert, to_insert)\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiofile\n",
      "  Downloading aiofile-3.8.8-py3-none-any.whl (19 kB)\n",
      "Collecting caio~=0.9.0\n",
      "  Downloading caio-0.9.13-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: caio, aiofile\n",
      "Successfully installed aiofile-3.8.8 caio-0.9.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install aiofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping.crawlers.crawler_oxy\n"
     ]
    }
   ],
   "source": [
    "from Scraping import *\n",
    "\n",
    "cw = crawlers.CrawlerMaster(1, 'cmpg.oxy.elotech.com.br', 4119905, 'Ponta Grossa/PR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4119905, '1/2015/6', 6429),\n",
       " (4119905, '1/2015/7', 6715),\n",
       " (4119905, '1/2015/9', 6729),\n",
       " (4119905, '1/2016/6', 6099),\n",
       " (4119905, '1/2016/7', 6139)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = cw.query_files(5)\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "rows = cw.query_files()\n",
    "\n",
    "i = 1\n",
    "to_update = []\n",
    "for row in rows:   \n",
    "    file = os.path.join(\"\", \"Data\", row[0].__str__(), row[1].replace(\"/\", \"-\"))\n",
    "    to_update.append((path, row[0], row[1], row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert = \"UPDATE FILES set path = ? where ibge = ? and procurement_id = ? and file_id = ?\"\n",
    "con = sqlite3.connect(\"scraped.db\", timeout=50)\n",
    "cursor = con.cursor()\n",
    "cursor.executemany(insert, to_update)\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validar pastas\n",
    "pastas = cw.query_folders()\n",
    "for pasta in pastas:\n",
    "    if not os.path.exists(pasta[0]):\n",
    "        os.makedirs(pasta[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://cmpg.oxy.elotech.com.br/portaltransparencia-api/api/files/arquivo/6429',\n",
       "  4119905,\n",
       "  '1/2015/6',\n",
       "  6429,\n",
       "  'Data\\\\4119905\\\\1-2015-6',\n",
       "  '6429.pdf'),\n",
       " ('https://cmpg.oxy.elotech.com.br/portaltransparencia-api/api/files/arquivo/6715',\n",
       "  4119905,\n",
       "  '1/2015/7',\n",
       "  6715,\n",
       "  'Data\\\\4119905\\\\1-2015-7',\n",
       "  '6715.pdf'),\n",
       " ('https://cmpg.oxy.elotech.com.br/portaltransparencia-api/api/files/arquivo/6729',\n",
       "  4119905,\n",
       "  '1/2015/9',\n",
       "  6729,\n",
       "  'Data\\\\4119905\\\\1-2015-9',\n",
       "  '6729.pdf')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = cw.query_files(3)\n",
    "files2 = [('https://{base}/portaltransparencia-api/api/files/arquivo/{file_id}'.format(\n",
    "                base=cw._url,\n",
    "                file_id=x[2]\n",
    "            ), ) + x for x in files]\n",
    "files2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://cmpg.oxy.elotech.com.br/portaltransparencia-api/api/files/arquivo/6729', 4119905, '1/2015/9', 6729, 'Data\\\\4119905\\\\1-2015-9', '6729.pdf')\n"
     ]
    }
   ],
   "source": [
    "for file in files2:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest-asyncio in c:\\users\\danie\\onedrive\\7 - estudos\\3 - mba usp 2022\\6 - tcc\\code\\govbrprocurement\\venv\\lib\\site-packages (1.5.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if file exists\n",
    "files3 = [x for x in files2 if not os.path.isfile(os.path.join(x[4], x[5]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baixar e salvar cada arquivo\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp  # pip install aiohttp\n",
    "import aiofile  # pip install aiofile\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def download_files_from_report(files):\n",
    "\n",
    "    #sema = asyncio.BoundedSemaphore(5)\n",
    "\n",
    "    async def fetch_file(session, file):\n",
    "        \n",
    "        url = file[0]\n",
    "        folder = file[4]\n",
    "        fname = file[5]\n",
    "\n",
    "        #async with sema:\n",
    "        async with session.get(url) as resp:\n",
    "            assert resp.status == 200\n",
    "            data = await resp.read()\n",
    "\n",
    "        async with aiofile.async_open(\n",
    "            os.path.join(folder, fname), \"wb\"\n",
    "        ) as outfile:\n",
    "            await outfile.write(data)\n",
    "\n",
    "    async def main():\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = [fetch_file(session, file) for file in files]\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main())\n",
    "    #loop.close()\n",
    "\n",
    "download_files_from_report(files3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping.crawlers.crawler_oxy\n",
      "there is 2483 files in the database\n",
      "there is 1827 files to be downloaded\n",
      "the first file id is: 6139.\n",
      "there is 2483 files in the database\n",
      "there is 0 files to be downloaded\n",
      "there is 2483 files in the database\n",
      "there is 0 files to be downloaded\n",
      "there is 2483 files in the database\n",
      "there is 0 files to be downloaded\n",
      "there is 2483 files in the database\n",
      "there is 0 files to be downloaded\n",
      "there is 2483 files in the database\n",
      "there is 0 files to be downloaded\n",
      "there is 2483 files in the database\n",
      "there is 0 files to be downloaded\n",
      "there is 2483 files in the database\n",
      "there is 0 files to be downloaded\n",
      "there is 2483 files in the database\n",
      "there is 0 files to be downloaded\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mcw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\Scraping\\crawlers\\crawler_oxy.py:120\u001b[0m, in \u001b[0;36mCrawlerOxy.get_all_files\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m files3 \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m files2 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(x[\u001b[38;5;241m4\u001b[39m], x[\u001b[38;5;241m5\u001b[39m]))]\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthere is \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m files to be downloaded\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(files3\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m()))\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe first file id is: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mfiles3\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m3\u001b[39m]))\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m#Download and save files\u001b[39;00m\n\u001b[0;32m    124\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from Scraping import *\n",
    "\n",
    "cw = crawlers.CrawlerMaster(1, 'cmpg.oxy.elotech.com.br', 4119905, 'Ponta Grossa/PR')\n",
    "try:\n",
    "    cw.get_all_files()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IronPdf in c:\\users\\danie\\onedrive\\7 - estudos\\3 - mba usp 2022\\6 - tcc\\code\\govbrprocurement\\venv\\lib\\site-packages (2023.8.6)\n",
      "Requirement already satisfied: pythonnet in c:\\users\\danie\\onedrive\\7 - estudos\\3 - mba usp 2022\\6 - tcc\\code\\govbrprocurement\\venv\\lib\\site-packages (from IronPdf) (3.0.3)\n",
      "Requirement already satisfied: clr-loader<0.3.0,>=0.2.6 in c:\\users\\danie\\onedrive\\7 - estudos\\3 - mba usp 2022\\6 - tcc\\code\\govbrprocurement\\venv\\lib\\site-packages (from pythonnet->IronPdf) (0.2.6)\n",
      "Requirement already satisfied: cffi>=1.13 in c:\\users\\danie\\onedrive\\7 - estudos\\3 - mba usp 2022\\6 - tcc\\code\\govbrprocurement\\venv\\lib\\site-packages (from clr-loader<0.3.0,>=0.2.6->pythonnet->IronPdf) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\danie\\onedrive\\7 - estudos\\3 - mba usp 2022\\6 - tcc\\code\\govbrprocurement\\venv\\lib\\site-packages (from cffi>=1.13->clr-loader<0.3.0,>=0.2.6->pythonnet->IronPdf) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install IronPdf --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## __init__ crawlers load: oxy\n",
      "## Starting Scraping.crawlers.crawler_oxy ##\n",
      "## Starting Scraping.crawlers.crawler_base ##\n",
      "## __init__ crawlers load: base\n",
      "## __init__ crawlers load: master\n",
      "## Starting Scraping.crawlers.crawler_master ##\n",
      "[(4119905, '1/2015/6', 6429, 'Files\\\\4119905\\\\1-2015-6', '6429.pdf'), (4119905, '1/2015/7', 6715, 'Files\\\\4119905\\\\1-2015-7', '6715.pdf')]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Files\\\\4119905\\\\1-2015-6\\\\6429.pdf\\\\temp_image_0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[0;32m      6\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\Scraping\\crawlers\\crawler_base.py:116\u001b[0m, in \u001b[0;36mCrawlerBase.do_ocr\u001b[1;34m(self, limit)\u001b[0m\n\u001b[0;32m    112\u001b[0m     con\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    114\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun(main())\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\venv\\Lib\\site-packages\\nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\venv\\Lib\\site-packages\\nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:269\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\Scraping\\crawlers\\crawler_base.py:103\u001b[0m, in \u001b[0;36mCrawlerBase.do_ocr.<locals>.main\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# ibge=row[0], procurement_id=row[1], file_id=row[2], path=row[3], new_name=row[4]\u001b[39;00m\n\u001b[0;32m    101\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_files(rows)\n\u001b[1;32m--> 103\u001b[0m to_update \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(rows, results):\n\u001b[0;32m    105\u001b[0m     to_update\u001b[38;5;241m.\u001b[39mappend(result, row[\u001b[38;5;241m0\u001b[39m], row[\u001b[38;5;241m1\u001b[39m], row[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\Scraping\\crawlers\\crawler_base.py:95\u001b[0m, in \u001b[0;36mCrawlerBase.do_ocr.<locals>.process_files\u001b[1;34m(rows)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_files\u001b[39m(rows):\n\u001b[0;32m     94\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [get_text_async(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(row[\u001b[38;5;241m3\u001b[39m], row[\u001b[38;5;241m4\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows]\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:339\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\Scraping\\crawlers\\crawler_base.py:82\u001b[0m, in \u001b[0;36mCrawlerBase.do_ocr.<locals>.get_text_async\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images):\n\u001b[0;32m     81\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdf_path,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_image_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m     \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPNG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     image_paths\u001b[38;5;241m.\u001b[39mappend(image_path)\n\u001b[0;32m     85\u001b[0m text_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_pages(image_paths)\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\7 - Estudos\\3 - MBA USP 2022\\6 - TCC\\Code\\GovBrProcurement\\venv\\Lib\\site-packages\\PIL\\Image.py:2435\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2433\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2434\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2435\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2438\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Files\\\\4119905\\\\1-2015-6\\\\6429.pdf\\\\temp_image_0.png'"
     ]
    }
   ],
   "source": [
    "from Scraping import *\n",
    "\n",
    "cw = crawlers.CrawlerMaster(1, 'cmpg.oxy.elotech.com.br', 4119905, 'Ponta Grossa/PR')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "cw.do_ocr(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
